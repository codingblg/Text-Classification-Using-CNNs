{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to understand Neural Network, Deep Learning, and Convolutional Neural Networks(CNNs).\n",
    "I am designing a CNNs on the resume data sets to identify their job title based on the job descriptions in the resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\ligang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from subprocess import check_output\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 # word vector dimension, as in the golve\n",
    "MAX_VOCAB_SIZE = 20000 # number of unique words\n",
    "MAX_SEQUENCE_LENGTH = 200 # number of words in a job description\n",
    "\n",
    "# training params\n",
    "batch_size = 256\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Created data conversion requirements based on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Wrote a python based SQL generator that helped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Lead</td>\n",
       "      <td>Implement natural language processing and mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Established a new data analysis pipeline with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Series D peer to peer car rental service Devel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0               Data Scientist   \n",
       "1               Data Scientist   \n",
       "2  Senior Data Scientist, Lead   \n",
       "3               Data Scientist   \n",
       "4               Data Scientist   \n",
       "\n",
       "                                         description  \n",
       "0   Created data conversion requirements based on...  \n",
       "1  Wrote a python based SQL generator that helped...  \n",
       "2  Implement natural language processing and mach...  \n",
       "3   Established a new data analysis pipeline with...  \n",
       "4  Series D peer to peer car rental service Devel...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"result.csv\")\n",
    "data['description'] = data['description'].astype('str')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8031</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Collaborated with corporate IT to redesign an ...</td>\n",
       "      <td>[Collaborated, corporate, IT, redesign, enterp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ConnectOne Bank is an FDIC insured institution...</td>\n",
       "      <td>[ConnectOne, Bank, FDIC, insured, institution,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>Data Scientist (Contract)</td>\n",
       "      <td>Responsibility Developed Relational Database u...</td>\n",
       "      <td>[Responsibility, Developed, Relational, Databa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Responsibilities Coordinate with the business ...</td>\n",
       "      <td>[Responsibilities, Coordinate, business, users...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>DATA SCIENTIST / BIG DATA ANALYST</td>\n",
       "      <td>Developed Product Management Channel Strategy...</td>\n",
       "      <td>[Developed, Product, Management, Channel, Stra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>DATA ANALYST</td>\n",
       "      <td>Created engaging and intuitive dashboards to p...</td>\n",
       "      <td>[Created, engaging, intuitive, dashboards, pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>Sr. Modeler/ Data Scientist</td>\n",
       "      <td>Responsibilities Project Lead on Predictive Mo...</td>\n",
       "      <td>[Responsibilities, Project, Lead, Predictive, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Machine Damage Identification Improved the ac...</td>\n",
       "      <td>[Machine, Damage, Identification, Improved, ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>Data Scientist/Asst. Manager</td>\n",
       "      <td>Kolkata India Jan Jul Role Data Scientist Asst...</td>\n",
       "      <td>[Kolkata, India, Jan, Jul, Role, Data, Scienti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>Sr. Data Scientist/Machine Learning Engineer</td>\n",
       "      <td>Responsibilities Developed MapReduce Spark Pyt...</td>\n",
       "      <td>[Responsibilities, Developed, MapReduce, Spark...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  \\\n",
       "8031                                  Data Analyst   \n",
       "1114                                Data Scientist   \n",
       "2864                     Data Scientist (Contract)   \n",
       "7013                                  Data Analyst   \n",
       "1481             DATA SCIENTIST / BIG DATA ANALYST   \n",
       "8462                                  DATA ANALYST   \n",
       "4991                   Sr. Modeler/ Data Scientist   \n",
       "5796                                Data Scientist   \n",
       "3137                  Data Scientist/Asst. Manager   \n",
       "2480  Sr. Data Scientist/Machine Learning Engineer   \n",
       "\n",
       "                                            description  \\\n",
       "8031  Collaborated with corporate IT to redesign an ...   \n",
       "1114  ConnectOne Bank is an FDIC insured institution...   \n",
       "2864  Responsibility Developed Relational Database u...   \n",
       "7013  Responsibilities Coordinate with the business ...   \n",
       "1481   Developed Product Management Channel Strategy...   \n",
       "8462  Created engaging and intuitive dashboards to p...   \n",
       "4991  Responsibilities Project Lead on Predictive Mo...   \n",
       "5796   Machine Damage Identification Improved the ac...   \n",
       "3137  Kolkata India Jan Jul Role Data Scientist Asst...   \n",
       "2480  Responsibilities Developed MapReduce Spark Pyt...   \n",
       "\n",
       "                                                 tokens  label  \n",
       "8031  [Collaborated, corporate, IT, redesign, enterp...      0  \n",
       "1114  [ConnectOne, Bank, FDIC, insured, institution,...      1  \n",
       "2864  [Responsibility, Developed, Relational, Databa...      1  \n",
       "7013  [Responsibilities, Coordinate, business, users...      0  \n",
       "1481  [Developed, Product, Management, Channel, Stra...      1  \n",
       "8462  [Created, engaging, intuitive, dashboards, pro...      0  \n",
       "4991  [Responsibilities, Project, Lead, Predictive, ...      1  \n",
       "5796  [Machine, Damage, Identification, Improved, ac...      1  \n",
       "3137  [Kolkata, India, Jan, Jul, Role, Data, Scienti...      1  \n",
       "2480  [Responsibilities, Developed, MapReduce, Spark...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['tokens'] = data['description'].apply(tokenizer.tokenize)\n",
    "data['tokens'] = data['tokens'].apply(lambda vec: [word for word in vec if word not in stop_words]) # remove stopwords\n",
    "data['label'] = data['title'].map(lambda x: 1 if 'Data Scientist' in x and 'Data Analyst' in x else 0) \n",
    "# remove samples where both data scientist and data analyst exist in the title\n",
    "data.drop(data[data.label==1].index, inplace = True)\n",
    "# label data scientist to 1 and data analyst to 0\n",
    "data['label'] = data['title'].map(lambda x: 1 if 'data scientist' in x.lower() else 0)\n",
    "\n",
    "# shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279585 words total, with a vocabulary size of 34147\n",
      "Max sentence length is 1542\n"
     ]
    }
   ],
   "source": [
    "# list all the words in the dataset\n",
    "all_training_words = [word for tokens in data['tokens'] for word in tokens]\n",
    "# count the number of words\n",
    "training_sentence_lengths = [len(tokens) for tokens in data['tokens']]\n",
    "# number of unique words\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" %(len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained glove vectors and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('e:/download/gensim_glove_vectors.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25475 unique tokens.\n",
      "(25476, 300)\n"
     ]
    }
   ],
   "source": [
    "# define a tokenizer, keep the most common words in the dataset\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "# train on the data['description']\n",
    "tokenizer.fit_on_texts(data['description'].tolist())\n",
    "# turn the text to sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(data['description'].tolist())\n",
    "# the word index\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "# padding the sequences(text) to the same length\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# mapping the sequence(text) to glove vector\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word, index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Convolutional Neural Network following Yoon Kim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words, embedding_dim, weights=[embeddings], input_length=max_sequence_length,\n",
    "                                trainable=trainable)\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    \n",
    "    l_merge = keras.layers.Concatenate(axis=1)(convs)\n",
    "    \n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "    \n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     7642800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 128)     192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 66, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 65, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 196, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 196, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25088)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          3211392     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,315,505\n",
      "Trainable params: 3,672,705\n",
      "Non-trainable params: 7,642,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM,\n",
    "                1, False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8130 samples, validate on 2033 samples\n",
      "Epoch 1/10\n",
      "8130/8130 [==============================] - 112s 14ms/step - loss: 0.9600 - acc: 0.6149 - val_loss: 0.6160 - val_acc: 0.6640\n",
      "Epoch 2/10\n",
      "8130/8130 [==============================] - 109s 13ms/step - loss: 0.5802 - acc: 0.6775 - val_loss: 0.4870 - val_acc: 0.7890\n",
      "Epoch 3/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.4846 - acc: 0.7798 - val_loss: 0.4323 - val_acc: 0.8165\n",
      "Epoch 4/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.4102 - acc: 0.8290 - val_loss: 0.4227 - val_acc: 0.8146\n",
      "Epoch 5/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.3474 - acc: 0.8562 - val_loss: 0.3965 - val_acc: 0.8337\n",
      "Epoch 6/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.2866 - acc: 0.8898 - val_loss: 0.4042 - val_acc: 0.8342\n",
      "Epoch 7/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.2126 - acc: 0.9239 - val_loss: 0.4313 - val_acc: 0.8293\n",
      "Epoch 8/10\n",
      "8130/8130 [==============================] - 109s 13ms/step - loss: 0.1571 - acc: 0.9419 - val_loss: 0.4628 - val_acc: 0.8288\n",
      "Epoch 9/10\n",
      "8130/8130 [==============================] - 108s 13ms/step - loss: 0.1195 - acc: 0.9608 - val_loss: 0.4995 - val_acc: 0.8318\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.2,\n",
    "                shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
